# Recon-Try-Exploit-My-Note

1. Passive Recon

`subfinder` -->  `git clone https://github.com/projectdiscovery/subfinder.git` `go install -v github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest`
    [$subfinder -d xsolla.com -silent | tee xsolla.com.txt]
`assetfinder`
`findomin`
`amass`


2. Active Recon

`puredns` `go install github.com/d3mondev/puredns/v2@latest`
[
puredns resolve domains.txt
puredns bruteforce wordlist.txt domain.com --resolvers public.txt
cat domains.txt | puredns resolve 
]
`massdns` `https://github.com/blechschmidt/massdns`

check all subdomain --> https://httpstatus.io/

3. permutation Recon
https://blog.projectdiscovery.io/introducing-alterx-simplifying-active-subdomain-enumeration-with-patterns/

`gotator`  `go install github.com/Josue87/gotator@latest` `gotator -sub xsolla.com.txt -perm /usr/share/wordlists/amass/all.txt -mindup -t 100 | tee gotator-xsolla.com.final1.txt`
`puredns`
`ripgen`

`gotator` result when found then run `httpx` --> `cat hosts.txt | httpx` --> `httpx -list hosts.txt -silent -probe` --> `echo 173.0.84.0/24 | httpx -silent` --> `subfinder -d hackerone.com -silent| httpx -title -tech-detect -status-code`



4. Content Discovery  

https://medium.com/@nynan/bug-bounty-recon-content-discovery-efficiency-pays-2ec2462532b1
https://exploit-notes.hdks.org/exploit/web/method/web-content-discovery/

`ParamSpider`

`ffuf` 
{--!!

    ![ffuf mascot](_img/ffuf_run_logo_600.png)
# ffuf - Fuzz Faster U Fool

A fast web fuzzer written in Go.

- [Installation](https://github.com/ffuf/ffuf#installation)
- [Example usage](https://github.com/ffuf/ffuf#example-usage)
    - [Content discovery](https://github.com/ffuf/ffuf#typical-directory-discovery)
    - [Vhost discovery](https://github.com/ffuf/ffuf#virtual-host-discovery-without-dns-records)
    - [Parameter fuzzing](https://github.com/ffuf/ffuf#get-parameter-fuzzing)
    - [POST data fuzzing](https://github.com/ffuf/ffuf#post-data-fuzzing)
    - [Using external mutator](https://github.com/ffuf/ffuf#using-external-mutator-to-produce-test-cases)
    - [Configuration files](https://github.com/ffuf/ffuf#configuration-files)
- [Help](https://github.com/ffuf/ffuf#usage)
    - [Interactive mode](https://github.com/ffuf/ffuf#interactive-mode)


## Installation

- [Download](https://github.com/ffuf/ffuf/releases/latest) a prebuilt binary from [releases page](https://github.com/ffuf/ffuf/releases/latest), unpack and run!

  _or_
- If you are on macOS with [homebrew](https://brew.sh), ffuf can be installed with: `brew install ffuf`
  
  _or_
- If you have recent go compiler installed: `go install github.com/ffuf/ffuf/v2@latest` (the same command works for updating)
  
  _or_
- `git clone https://github.com/ffuf/ffuf ; cd ffuf ; go get ; go build`

Ffuf depends on Go 1.16 or greater.

## Example usage

The usage examples below show just the simplest tasks you can accomplish using `ffuf`. 

More elaborate documentation that goes through many features with a lot of examples is
available in the ffuf wiki at [https://github.com/ffuf/ffuf/wiki](https://github.com/ffuf/ffuf/wiki)

For more extensive documentation, with real life usage examples and tips, be sure to check out the awesome guide:
"[Everything you need to know about FFUF](https://codingo.io/tools/ffuf/bounty/2020/09/17/everything-you-need-to-know-about-ffuf.html)" by 
Michael Skelton ([@codingo](https://github.com/codingo)).

You can also practise your ffuf scans against a live host with different lessons and use cases either locally by using the docker container https://github.com/adamtlangley/ffufme or against the live hosted version at http://ffuf.me created by Adam Langley [@adamtlangley](https://twitter.com/adamtlangley).  

### Typical directory discovery

[![asciicast](https://asciinema.org/a/211350.png)](https://asciinema.org/a/211350)

By using the FUZZ keyword at the end of URL (`-u`):

```
ffuf -w /path/to/wordlist -u https://target/FUZZ
```

### Virtual host discovery (without DNS records)

[![asciicast](https://asciinema.org/a/211360.png)](https://asciinema.org/a/211360)

Assuming that the default virtualhost response size is 4242 bytes, we can filter out all the responses of that size (`-fs 4242`)while fuzzing the Host - header:

```
ffuf -w /path/to/vhost/wordlist -u https://target -H "Host: FUZZ" -fs 4242
```

### GET parameter fuzzing

GET parameter name fuzzing is very similar to directory discovery, and works by defining the `FUZZ` keyword as a part of the URL. This also assumes a response size of 4242 bytes for invalid GET parameter name.

```
ffuf -w /path/to/paramnames.txt -u https://target/script.php?FUZZ=test_value -fs 4242
```

If the parameter name is known, the values can be fuzzed the same way. This example assumes a wrong parameter value returning HTTP response code 401.

```
ffuf -w /path/to/values.txt -u https://target/script.php?valid_name=FUZZ -fc 401
```

### POST data fuzzing

This is a very straightforward operation, again by using the `FUZZ` keyword. This example is fuzzing only part of the POST request. We're again filtering out the 401 responses.

```
ffuf -w /path/to/postdata.txt -X POST -d "username=admin\&password=FUZZ" -u https://target/login.php -fc 401
```

### Maximum execution time

If you don't want ffuf to run indefinitely, you can use the `-maxtime`. This stops __the entire__ process after a given time (in seconds).

```
ffuf -w /path/to/wordlist -u https://target/FUZZ -maxtime 60
```

When working with recursion, you can control the maxtime __per job__ using `-maxtime-job`. This will stop the current job after a given time (in seconds) and continue with the next one. New jobs are created when the recursion functionality detects a subdirectory.

```
ffuf -w /path/to/wordlist -u https://target/FUZZ -maxtime-job 60 -recursion -recursion-depth 2
```

It is also possible to combine both flags limiting the per job maximum execution time as well as the overall execution time. If you do not use recursion then both flags behave equally.

### Using external mutator to produce test cases

For this example, we'll fuzz JSON data that's sent over POST. [Radamsa](https://gitlab.com/akihe/radamsa) is used as the mutator.

When `--input-cmd` is used, ffuf will display matches as their position. This same position value will be available for the callee as an environment variable `$FFUF_NUM`. We'll use this position value as the seed for the mutator. Files example1.txt and example2.txt contain valid JSON payloads. We are matching all the responses, but filtering out response code `400 - Bad request`:

```
ffuf --input-cmd 'radamsa --seed $FFUF_NUM example1.txt example2.txt' -H "Content-Type: application/json" -X POST -u https://ffuf.io.fi/FUZZ -mc all -fc 400
```

It of course isn't very efficient to call the mutator for each payload, so we can also pre-generate the payloads, still using [Radamsa](https://gitlab.com/akihe/radamsa) as an example:

```
# Generate 1000 example payloads
radamsa -n 1000 -o %n.txt example1.txt example2.txt

# This results into files 1.txt ... 1000.txt
# Now we can just read the payload data in a loop from file for ffuf

ffuf --input-cmd 'cat $FFUF_NUM.txt' -H "Content-Type: application/json" -X POST -u https://ffuf.io.fi/ -mc all -fc 400
```

### Configuration files

When running ffuf, it first checks if a default configuration file exists. Default path for a `ffufrc` file is
`$XDG_CONFIG_HOME/ffuf/ffufrc`.  You can configure one or multiple options in this file, and they will be applied on 
every subsequent ffuf job. An example of ffufrc file can be found 
[here](https://github.com/ffuf/ffuf/blob/master/ffufrc.example). 

A more detailed description about configuration file locations can be found in the wiki: 
[https://github.com/ffuf/ffuf/wiki/Configuration](https://github.com/ffuf/ffuf/wiki/Configuration)

The configuration options provided on the command line override the ones loaded from the default `ffufrc` file.
Note: this does not apply for CLI flags that can be provided more than once. One of such examples is `-H` (header) flag.
In this case, the `-H` values provided on the command line will be _appended_ to the ones from the config file instead.

Additionally, in case you wish to use bunch of configuration files for different use cases, you can do this by defining
the configuration file path using `-config` command line flag that takes the file path to the configuration file as its
parameter. 

<p align="center">
  <img width="250" src="_img/ffuf_juggling_250.png">
</p>

## Usage

To define the test case for ffuf, use the keyword `FUZZ` anywhere in the URL (`-u`), headers (`-H`), or POST data (`-d`).

```
Fuzz Faster U Fool - v2.0.0

HTTP OPTIONS:
  -H                  Header `"Name: Value"`, separated by colon. Multiple -H flags are accepted.
  -X                  HTTP method to use
  -b                  Cookie data `"NAME1=VALUE1; NAME2=VALUE2"` for copy as curl functionality.
  -d                  POST data
  -http2              Use HTTP2 protocol (default: false)
  -ignore-body        Do not fetch the response content. (default: false)
  -r                  Follow redirects (default: false)
  -recursion          Scan recursively. Only FUZZ keyword is supported, and URL (-u) has to end in it. (default: false)
  -recursion-depth    Maximum recursion depth. (default: 0)
  -recursion-strategy Recursion strategy: "default" for a redirect based, and "greedy" to recurse on all matches (default: default)
  -replay-proxy       Replay matched requests using this proxy.
  -sni                Target TLS SNI, does not support FUZZ keyword
  -timeout            HTTP request timeout in seconds. (default: 10)
  -u                  Target URL
  -x                  Proxy URL (SOCKS5 or HTTP). For example: http://127.0.0.1:8080 or socks5://127.0.0.1:8080

GENERAL OPTIONS:
  -V                  Show version information. (default: false)
  -ac                 Automatically calibrate filtering options (default: false)
  -acc                Custom auto-calibration string. Can be used multiple times. Implies -ac
  -ach                Per host autocalibration (default: false)
  -ack                Autocalibration keyword (default: FUZZ)
  -acs                Autocalibration strategy: "basic" or "advanced" (default: basic)
  -c                  Colorize output. (default: false)
  -config             Load configuration from a file
  -json               JSON output, printing newline-delimited JSON records (default: false)
  -maxtime            Maximum running time in seconds for entire process. (default: 0)
  -maxtime-job        Maximum running time in seconds per job. (default: 0)
  -noninteractive     Disable the interactive console functionality (default: false)
  -p                  Seconds of `delay` between requests, or a range of random delay. For example "0.1" or "0.1-2.0"
  -rate               Rate of requests per second (default: 0)
  -s                  Do not print additional information (silent mode) (default: false)
  -sa                 Stop on all error cases. Implies -sf and -se. (default: false)
  -scraperfile        Custom scraper file path
  -scrapers           Active scraper groups (default: all)
  -se                 Stop on spurious errors (default: false)
  -search             Search for a FFUFHASH payload from ffuf history
  -sf                 Stop when > 95% of responses return 403 Forbidden (default: false)
  -t                  Number of concurrent threads. (default: 40)
  -v                  Verbose output, printing full URL and redirect location (if any) with the results. (default: false)

MATCHER OPTIONS:
  -mc                 Match HTTP status codes, or "all" for everything. (default: 200,204,301,302,307,401,403,405,500)
  -ml                 Match amount of lines in response
  -mmode              Matcher set operator. Either of: and, or (default: or)
  -mr                 Match regexp
  -ms                 Match HTTP response size
  -mt                 Match how many milliseconds to the first response byte, either greater or less than. EG: >100 or <100
  -mw                 Match amount of words in response

FILTER OPTIONS:
  -fc                 Filter HTTP status codes from response. Comma separated list of codes and ranges
  -fl                 Filter by amount of lines in response. Comma separated list of line counts and ranges
  -fmode              Filter set operator. Either of: and, or (default: or)
  -fr                 Filter regexp
  -fs                 Filter HTTP response size. Comma separated list of sizes and ranges
  -ft                 Filter by number of milliseconds to the first response byte, either greater or less than. EG: >100 or <100
  -fw                 Filter by amount of words in response. Comma separated list of word counts and ranges

INPUT OPTIONS:
  -D                  DirSearch wordlist compatibility mode. Used in conjunction with -e flag. (default: false)
  -e                  Comma separated list of extensions. Extends FUZZ keyword.
  -ic                 Ignore wordlist comments (default: false)
  -input-cmd          Command producing the input. --input-num is required when using this input method. Overrides -w.
  -input-num          Number of inputs to test. Used in conjunction with --input-cmd. (default: 100)
  -input-shell        Shell to be used for running command
  -mode               Multi-wordlist operation mode. Available modes: clusterbomb, pitchfork, sniper (default: clusterbomb)
  -request            File containing the raw http request
  -request-proto      Protocol to use along with raw request (default: https)
  -w                  Wordlist file path and (optional) keyword separated by colon. eg. '/path/to/wordlist:KEYWORD'

OUTPUT OPTIONS:
  -debug-log          Write all of the internal logging to the specified file.
  -o                  Write output to file
  -od                 Directory path to store matched results to.
  -of                 Output file format. Available formats: json, ejson, html, md, csv, ecsv (or, 'all' for all formats) (default: json)
  -or                 Don't create the output file if we don't have results (default: false)

EXAMPLE USAGE:
  Fuzz file paths from wordlist.txt, match all responses but filter out those with content-size 42.
  Colored, verbose output.
    ffuf -w wordlist.txt -u https://example.org/FUZZ -mc all -fs 42 -c -v

  Fuzz Host-header, match HTTP 200 responses.
    ffuf -w hosts.txt -u https://example.org/ -H "Host: FUZZ" -mc 200

  Fuzz POST JSON data. Match all responses not containing text "error".
    ffuf -w entries.txt -u https://example.org/ -X POST -H "Content-Type: application/json" \
      -d '{"name": "FUZZ", "anotherkey": "anothervalue"}' -fr "error"

  Fuzz multiple locations. Match only responses reflecting the value of "VAL" keyword. Colored.
    ffuf -w params.txt:PARAM -w values.txt:VAL -u https://example.org/?PARAM=VAL -mr "VAL" -c

  More information and examples: https://github.com/ffuf/ffuf
```

### Interactive mode

By pressing `ENTER` during ffuf execution, the process is paused and user is dropped to a shell-like interactive mode:
```
entering interactive mode
type "help" for a list of commands, or ENTER to resume.
> help

available commands:
 afc  [value]             - append to status code filter 
 fc   [value]             - (re)configure status code filter 
 afl  [value]             - append to line count filter 
 fl   [value]             - (re)configure line count filter 
 afw  [value]             - append to word count filter 
 fw   [value]             - (re)configure word count filter 
 afs  [value]             - append to size filter 
 fs   [value]             - (re)configure size filter 
 aft  [value]             - append to time filter 
 ft   [value]             - (re)configure time filter 
 rate [value]             - adjust rate of requests per second (active: 0)
 queueshow                - show job queue
 queuedel [number]        - delete a job in the queue
 queueskip                - advance to the next queued job
 restart                  - restart and resume the current ffuf job
 resume                   - resume current ffuf job (or: ENTER) 
 show                     - show results for the current job
 savejson [filename]      - save current matches to a file
 help                     - you are looking at it
> 
```

in this mode, filters can be reconfigured, queue managed and the current state saved to disk.

When (re)configuring the filters, they get applied posthumously and all the false positive matches from memory that
would have been filtered out by the newly added filters get deleted.

The new state of matches can be printed out with a command `show` that will print out all the matches as like they 
would have been found by `ffuf`.

As "negative" matches are not stored to memory, relaxing the filters cannot unfortunately bring back the lost matches.
For this kind of scenario, the user is able to use the command `restart`, which resets the state and starts the current
job from the beginning.

<p align="center">
  <img width="250" src="_img/ffuf_waving_250.png">
</p>

## License

ffuf is released under MIT license. See [LICENSE](https://github.com/ffuf/ffuf/blob/master/LICENSE).

!--}


`arjun`
---------Active Discovery----------
 
 `gobuster`
  `feroxbuster` {Brute Force, the Correct Way.endpoints,URLs, new parameters, and files/resources}
 
 `meg` { Meg Another thing most hunter’s overlook: websites don’t like being spammed with requests.If you send to many consecutive requests, you may be blocked, filtered, or be given false results. Which will slow you down and waste your time.In order to avoid annoying a server, we can use the tool meg by the amazing TomNomNom }
 
---Self Crawling---
 
 `hakrawler`

---------Passive Discovery---------
`go spider`
 `gau` 
 `WayBackMachine` 
 `waybackurls`

Existing Databases: AlienVault’s OTX and URLScan.io
`AlienVault’s OTX` https://otx.alienvault.com/
 and 
`URLscan.io` https://urlscan.io/
 are vast open source security tools which also have datasets that can be queried. AlientVault’s OTX and URLScan is included in gau.

----Google Dorks----
`site:example.com filetype:bak`  — backups, source code, databases, logs, etc.

Demonstration of using google dorks to find secrets
`site:example.com filetype:mdf`  — SQL databases
`site:example.com filetype:zip`  — source code, large datasets, anything. (Other compression extensions apply here too!)
`site:example.com filetype:sql`  — SQL related files
`site:example.com filetype:db`   — Generic databases
`site:example.com filetype:pdf`  — Internal secret reports
Sometimes we can find this stuff without even looking at the target site! Developers use lots of third party tools for organisation, and sometime reveal to much.

`site:trello.com “target”`       — secret endpoints, PII, etc

Demonstration of using a trello google dork to find secret information about an internal API.
`site:pastebin.com “target” `    — leaks
`site:codepen.io “target” `      — source code.

5. Web-Screenshot

`aquatone`
`gowitness` 
`webscreenshot`

----Essential Utility Tools-----
Very briefly we’ll cover some essential tools which you will need, now that you have endpoints.

`GF` By Tomnomnom — This allows you to sorts URLs into different vulnerability groups, letting you find bugs more easily.

`httpx` by ProjectDiscovery — This tells you which URLs are live (lot’s of urls from these methods will be dead). `https://github.com/projectdiscovery/httpx`

`uniq/sort` — cat subdomains.txt | sort | uniq  - will allow you to remove duplicate lines from files, you will get duplicate subdomains from various sources.


----------Manual Discovery----------

# Settings files
/robots.txt
/security.txt
/.well-known/security.txt
/.well-known/apple-app-site-association
/.well-known/assetlinks.json
/sitemap.xml
/sitemaps.xml

# JavaScript files
/main.js
/script.js
/js/jquery.min.js
/js/main.js
/js/script.js

# CGI scripts
/cgi-bin/example.cgi

# Wave dashes
/~files/
/~hidden/

# PHP files
/index.php
/config.php
/403.php
/404.php

# Python files
/main.py
/module.py
/module/__init__.py
/modules/__init__.py
__init__.py
config.ini
project.wsgi

# Archives
/example.zip
/backup.zip
/backups.zip

# Backup files
/example.bak
/example.jpg.bak
/images/example.jpg.bak

# Directories
/admin/
/blog/

# Sensitive information
/.env

# GitHub
/README.md
/.git
/.github
/.gitignore

# Apache Tomcat
/manager

# ASP.NET
/example.asp
/example.aspx

# If you know the users manage the website, try the usernames
/admin
/administrator
/john
/michael

# If we have the secret keyword found when investigating, we can attempt to access following contents.
/<keyword>
/<keyword>.html
/<keyword>.txt
/<keyword>.php
/<keyword>.py
/?<keyword>=test

# We might be able to access directories by using keywords we found.
/<site_title>
/<site_theme>
/<site_author>
/<image_theme>
/?<post_param>=test


# Wordlists
CeWL
CeWL is a curstom wordlist generator from websites.

`cewl https://example.com > wordlist.txt`
Copied!

SecLists
SecLists is a collection of multiple types of lists.
They are usually located in /usr/share/seclsits/ in Linux.

`less /usr/share/seclists/Discovery/Web-Content/common.txt`
Copied!

# Automation
Ffuf
For bug bounty programs, set the ‘-t’ flag and the ‘-p’ flag to decrease requests per second.

`ffuf -u https://example.com/.FUZZ -w wordlist.txt`
`ffuf -u https://example.com/FUZZ -w wordlist.txt `
`ffuf -u https://example.com/FUZZ.txt -w wordlist.txt`
`ffuf -u https://example.com/FUZZ.php -w wordlist.txt`
`ffuf -u https://example.com/index.php?FUZZ=test -w wordlist.txt`
# POST request
`ffuf -u https://example.com/FUZZ -X POST -w wordlist.txt`

# -t: Threads e.g. 5 threads
# -p: Pause N seconds per request e.g. 0.1 seconds
`ffuf -u http://example.com/FFUF -w wordlist.txt -t 5 -p 0.1`

# Custom header (-H)
`ffuf -H "Cookie: key=value" -u https://example.com/FUZZ -w wordlist.txt `

# -mc: Match HTTP statuc code
`ffuf -u http://example.com/FUZZ -w wordlist.txt -mc 200`
# 422 status code
`ffuf -u https://example.com/FUZZ -w wordlist.txt -mc 422`
# -ms: Match HTTP response size
`ffuf -u http://example.com/FUZZ -w wordlist.txt -ms 1234`
`ffuf -u http://example.com/FUZZ -w wordlist.txt -ms 50-300`

# -fc: Filter HTTP statuc code
`ffuf -u http://example.com/FUZZ -w wordlist.txt -fc 302`
# -fs: Filter HTTP response size
`ffuf -u http://example.com/FUZZ -w wordlist.txt -fs 1234`
`ffuf -u http://example.com/FUZZ -w wordlist.txt -fs 50-300`

# Extensions
`ffuf -u https://example.com/FUZZ -e .html,.txt,.js,.php,.py,.asp,.json -w wordlist.txt`
Copied!
For fuzzing with numbers, we can use the following commands.

`for i in {0..255}; do echo $i; done | ffuf -u 'http://example.com/?id=FUZZ' -w -`

`seq 0 255 | ffuf -u 'http://example.com/?id=FUZZ' -w -`
Copied!

# Dirsearch
Dirsearch is a web path scanner.
For bug bounty programs, set the flag “-t” and “—max-rate” to decrease requests per second.

`dirsearch -u https://example.com/`

# -w: wordlist
`dirsearch -u https://example.com/ -w wordlist.txt`

# -t: number of threads
# --max-rate: max requests per second
`dirsearch -u https://example.com/ -t 1 --max-rate=1`

# -m: Method
`dirsearch -m POST -u https://example.com/`

# Extensions
`dirsearch -u https://example.com -e html,txt,js,php,py,asp,json -w wordlist.txt`
Copied!

# Gobuster
`gobuster dir -u https://example.com -w wordlist.txt`
Copied!

# Dirb
`dirb https://example.com/`
`dirb https://example.com/ wordlist.txt`

# Custom header (-H)
`dirb https://example.com/ -H "Authorization: Basic {token}" wordlist.txt`
# File Extensions (-X)
`dirb https://example.com/ -X .txt`
Copied!

# FeroxBuster
FeroxBuster is a recursive content discovery.

`feroxbuster -u https://vulnerable.com`

# Specify extensions (-x)
`feroxbuster -u https://vulnerable.com -x html,js,php`
# No recursion (-n)
`feroxbuster -u https://vulnerable.com -n`
# Custom header (-H)
`feroxbuster -u https://vulnerable.com -H "Authorization: Bearer {token}"`
Copied!

# Hakrawler
Hakrawler is a simple web crawler designed for quick discovery of endpoints and assets within a web application.

`echo https://vulnerable.com | hakrawler`
Copied!

# Wfuzz
# -w: wordlist (alias for -z file,wordlist)
`wfuzz -w wordlist.txt https://example.com/FUZZ`
# -z: payload
`wfuzz -z file,wordlist.txt https://example.com/FUZZ`
Copied!

Framework Detection from Favicon
Get the information of the used framework from favicon.

`curl https://vulnerable.com/images/favicon.ico | md5sum`
Copied!
Then check what is the framework used in the website with the OWASP Favicon Database.


# Parsing .DS_Store
ds_store_exp is a tool that parses .DS_Store file and downloads files recursively.

`pip3 install ds-store`
`python3 ds_store_exp.py https://example.com/.DS_Store`



# Github
Github could be a great tool to gather information about a target’s infrastructure. We can simply search for a company name or website (like hackme.tld) to see the types of files and documents have been pushed to Github as a first step. We can also narrow down the search by using the information in our initial search to find more specific patterns. For example if we search for “hackme.tld” and find out that they are using “us.hackme.tld” for their internal apps like JIRA or their corporate VPN, that’s a good place to switch our focus to “us.hackme.tld” as our next search while we also do a subdomain brute force for anything behind “us.hackme.tld”. 

Github is also a great place to look for credentials and private API keys. The hardest thing about this method is getting creative when it comes down to looking for different keys. Here’s a list of a few items I look for on github:
`“Hackme.tld” API_key`
`“Hackme.tld” secret_key`
`“Hackme.tld” aws_key`
`“Hackme.tld” Password `
`“Hackme.tld” FTP`
`“Hackme.tld” login `
`“Hackme.tld” github_token `


# Amazon Web Services
AWS has become a huge asset to thousands of different companies. Some companies just use AWS s3 buckets to host their content, while others use it to deploy and host their application. Like any other resource, AWS could also be misconfigured or abused by criminals. For example, it's often possible to find misconfigured s3 buckets that allow a user outside of the organization to read and write files on a bucket belonging to that company.

These properties can be discovered by combining a few different methods: 
Using a google dork to find them: site:s3.amazonaws.com + hackme.tld
We can look them up on github: “hackme.tld” + “s3” 
We can bruteforce AWS to find specific s3 buckets and automate this to speed it up

`Lazys3` was developed based on method #3. This tool uses a list of commonly known s3 buckets, creates different patterns and permutations, grabs the response header, and returns them as long as it’s not returning 404. For example, if we are searching for s3 buckets under hackme.tld, this tool will run a test for each item of the wordlist, look up different patterns (i.e.: attachments-dev.hackme.tld, attachments.dev.hackme.tld, attachmentsdev.hackme.tld, and so on) and return those that have a response code of 200 or 403. 

While brute forcing and looking for different applications on Amazon Web Services may be help expand our attack surface, a few things could go wrong here as well:

The s3 bucket may hint that it belongs to a certain company, but in reality it’s not owned or operated by that company.
We may fall out of scope again due to different reasons including third party apps.
The s3 bucket may have read access, but none of files on the bucket contain any sensitive information.

Again, it's always good to check the scope, and keep in mind when reporting issues of this nature, they may get rejected if they fall into one of the above categories.

Asset Identification
To complement our previous methods, we can also use other asset identification tools such as Censys, Shodan, or archive.org to expand our attack surface. 

# Censys.io
Censys does a great job of scanning IP addresses and gathering information from a set of different ports. Censys could help us find more internal tools and assets by analyzing the SSL certificate belonging to that property. There are different methods and syntaxes to discover content using censys (https://censys.io/overview), but I personally just like to look at a target based on their SSL certs. For example, using the string `443.https.tls.certificate.parsed.extensions.subject_alt_name.dns_names:Yahoo.com` allows us to pull any subdomains/properties that point to Yahoo.com.

We can also tweak our searches on Censys just like any other tools. Creativity is a key asset while doing recon. There have been numerous times where I have searched censys for random patterns such as: “hackme.tld” + internal (or other keywords) that has resulted in finding unique properties that never showed up on my previous searches. 

img2

# Shodan.io
Shodan is similar to censys, except Shodan scans every IP address, finds any open ports on that IP address, and generates a ton of data and allows users to filter them by location, organization (owning that IP address), open ports, products (apache, tomcat, nginx, etc.), hostname, and more. 

That said, we can search for “hostname:hackme.tld org:hackme ports:15672” if we wanted to look for any RabbitMQ instances using a default port on hackme.tld. We can also change the ports option to “product” if we wanted to query for a specific product such as tomcat:  “hostname:hackme.tld org:hackme product:tomcat”

You can learn more about Shodan by reading the “Complete Guide to Shodan” written by Shodan’s founder, John Matherly.

# Archive.org
Archive.org is another great resource to find old robots.txt files that may contain old endpoints and other sensitive information, find older versions of the site in order to analyze the source and gather more information, and find other old and forgotten subdomains and dev environments.  We can do all of this by just visiting Archive.org, searching for our target, picking an older date (maybe from a year or two ago), and clicking around the website. 

This could also be automated (and it has been). Using waybackurl.py and waybackrobots.txt, we can find all the information above by running one of the scripts and waiting for the results. 
Archive.org is also a great place to find older javascript files that may have still be available to read from. Using this method, we can find more outdated functionalities and endpoints. 
After gathering a list of old and new javascript files, we can create a full list of all the endpoints mentioned in those javascript files using JSParser:





`gmapsapiscanner` for api key find that valid or not checker #tools ..



--!>Different indexes, such as Bing, Yandex, Duckduckgo
